{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1: What is K-Nearest Neighbors (KNN) and how does it work in both classification and regression problems?**\n",
        "\n",
        "Answer\n",
        "\n",
        " K-Nearest Neighbors (KNN) is a supervised, non-parametric, instance-based learning algorithm used for both classification and regression. It works by storing the training data and making predictions based on similarity. For a new data point, KNN calculates the distance (such as Euclidean distance) between the point and all training samples, then selects the K nearest neighbors.\n",
        "In classification, the predicted class is determined by majority voting among the neighbors.\n",
        "In regression, the prediction is the average (or weighted average) of the neighbors‚Äô values."
      ],
      "metadata": {
        "id": "38icBZ3J8ije"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2: What is the Curse of Dimensionality and how does it affect KNN performance?**\n",
        "\n",
        "Answer:-\n",
        "\n",
        "The Curse of Dimensionality refers to problems that arise when the number of features (dimensions) increases in a dataset. As dimensions grow, data points become sparse, and the distance between points becomes less meaningful.\n",
        "\n",
        "üîπ Effect on KNN Performance\n",
        "\n",
        "Distances between nearest and farthest neighbors become almost equal\n",
        "\n",
        "KNN struggles to identify truly ‚Äúnearest‚Äù neighbors\n",
        "\n",
        "Requires more data to maintain accuracy\n",
        "\n",
        "Leads to poor prediction performance and higher computation cost"
      ],
      "metadata": {
        "id": "q5JTVM1386Iu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3: What is Principal Component Analysis (PCA)? How is it different from feature selection?**\n",
        "\n",
        "Answer:- Principal Component Analysis (PCA) is an unsupervised dimensionality-reduction technique that transforms original correlated features into a smaller set of new, uncorrelated variables called principal components. These components capture the maximum variance present in the data while reducing dimensionality. PCA changes the feature space by combining features mathematically.\n",
        "\n",
        "Feature selection, in contrast, chooses a subset of the original features based on statistical tests, model importance, or domain knowledge, without transforming them.\n",
        "In summary, PCA creates new features, while feature selection retains existing ones, improving interpretability."
      ],
      "metadata": {
        "id": "UiQo4UUD9DwC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4: What are eigenvalues and eigenvectors in PCA, and why are they important?**\n",
        "\n",
        "Answer :- In Principal Component Analysis (PCA), eigenvectors represent the directions (axes) along which the data varies the most, while eigenvalues indicate the amount of variance captured along each eigenvector. They are computed from the covariance matrix of the data. Eigenvectors with larger eigenvalues correspond to more important principal components. PCA selects the top eigenvectors based on the highest eigenvalues to reduce dimensionality while preserving maximum information. Thus, eigenvectors define the new feature space, and eigenvalues help decide how many principal components to keep, ensuring minimal information loss."
      ],
      "metadata": {
        "id": "XJqAk_2W9P0c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5: How do KNN and PCA complement each other when applied in a single pipeline?**\n",
        "\n",
        "Answer:- KNN and PCA complement each other effectively in a single machine learning pipeline.\n",
        "KNN is a distance-based algorithm whose performance degrades in high-dimensional spaces due to the curse of dimensionality. PCA addresses this by reducing the number of features while preserving most of the data‚Äôs variance. By projecting data onto fewer, informative principal components, PCA removes noise, reduces sparsity, and makes distance calculations more meaningful. As a result, KNN becomes faster, more accurate, and less sensitive to irrelevant features. In practice, applying PCA before KNN improves computational efficiency and often leads to better classification or regression performance."
      ],
      "metadata": {
        "id": "pzKG9D6q9Z_3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6: Train a KNN Classifier on the Wine dataset with and without feature scaling. Compare model accuracy in both cases.**\n",
        "\n",
        "Answer:-"
      ],
      "metadata": {
        "id": "Kq_qW2Xb9j9S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MriEM2g48akf",
        "outputId": "eb465a02-f92e-49bb-aab3-84704d950ec7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.7407407407407407\n",
            "Accuracy with scaling: 0.9629629629629629\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Wine dataset\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# KNN WITHOUT Feature Scaling\n",
        "# -----------------------------\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "y_pred = knn.predict(X_test)\n",
        "accuracy_without_scaling = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# -----------------------------\n",
        "# KNN WITH Feature Scaling\n",
        "# -----------------------------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
        "accuracy_with_scaling = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Print results\n",
        "print(\"Accuracy without scaling:\", accuracy_without_scaling)\n",
        "print(\"Accuracy with scaling:\", accuracy_with_scaling)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7: Train a PCA model on the Wine dataset and print the explained variance ratio of each principal component. **\n",
        "\n",
        "Answer:-"
      ],
      "metadata": {
        "id": "8JBHHVuX-BiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load Wine dataset\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "# Standardize features (important for PCA)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply PCA (keep all components)\n",
        "pca = PCA()\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Print explained variance ratio\n",
        "print(\"Explained Variance Ratio of each Principal Component:\")\n",
        "print(pca.explained_variance_ratio_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDx_phW5-F-H",
        "outputId": "58dd8af4-bcc7-4eaf-a7c7-0ce0ec771668"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained Variance Ratio of each Principal Component:\n",
            "[0.36198848 0.1920749  0.11123631 0.0706903  0.06563294 0.04935823\n",
            " 0.04238679 0.02680749 0.02222153 0.01930019 0.01736836 0.01298233\n",
            " 0.00795215]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8: Train a KNN Classifier on the PCA-transformed dataset (retain top 2 components). Compare the accuracy with the original dataset.**\n",
        "\n",
        "Answer:-"
      ],
      "metadata": {
        "id": "jJPrUYre-Og3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# KNN on Original Dataset\n",
        "# -----------------------------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "y_pred_original = knn.predict(X_test_scaled)\n",
        "\n",
        "accuracy_original = accuracy_score(y_test, y_pred_original)\n",
        "\n",
        "# -----------------------------\n",
        "# PCA (Top 2 Components)\n",
        "# -----------------------------\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "y_pred_pca = knn_pca.predict(X_test_pca)\n",
        "\n",
        "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
        "\n",
        "# Print results\n",
        "print(\"Accuracy with Original Features:\", accuracy_original)\n",
        "print(\"Accuracy with PCA (2 Components):\", accuracy_pca)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxY5Kw9O-So2",
        "outputId": "3a069c0c-72be-4543-8c1d-bd16a7660d4d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Original Features: 0.9629629629629629\n",
            "Accuracy with PCA (2 Components): 0.9814814814814815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9: Train a KNN Classifier with different distance metrics (euclidean, manhattan) on the scaled Wine dataset and compare the results.**\n",
        "\n",
        "Answer:-"
      ],
      "metadata": {
        "id": "-9lHVIZG-dIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Wine dataset\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# -----------------------------\n",
        "# KNN with Euclidean Distance\n",
        "# -----------------------------\n",
        "knn_euclidean = KNeighborsClassifier(\n",
        "    n_neighbors=5, metric='euclidean'\n",
        ")\n",
        "knn_euclidean.fit(X_train_scaled, y_train)\n",
        "y_pred_euclidean = knn_euclidean.predict(X_test_scaled)\n",
        "accuracy_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
        "\n",
        "# -----------------------------\n",
        "# KNN with Manhattan Distance\n",
        "# -----------------------------\n",
        "knn_manhattan = KNeighborsClassifier(\n",
        "    n_neighbors=5, metric='manhattan'\n",
        ")\n",
        "knn_manhattan.fit(X_train_scaled, y_train)\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test_scaled)\n",
        "accuracy_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
        "\n",
        "# Print results\n",
        "print(\"Accuracy with Euclidean distance:\", accuracy_euclidean)\n",
        "print(\"Accuracy with Manhattan distance:\", accuracy_manhattan)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygqZ4hC7-h8K",
        "outputId": "1662c406-3ce3-4ecb-b423-4119d8366990"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Euclidean distance: 0.9629629629629629\n",
            "Accuracy with Manhattan distance: 0.9629629629629629\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You are working with a high-dimensional gene expression dataset to\n",
        "classify patients with different types of cancer.\n",
        "Due to the large number of features and a small number of samples, traditional models\n",
        "overfit.\n",
        "Explain how you would:\n",
        "‚óè Use PCA to reduce dimensionality\n",
        "‚óè Decide how many components to keep\n",
        "‚óè Use KNN for classification post-dimensionality reduction\n",
        "‚óè Evaluate the model\n",
        "‚óè Justify this pipeline to your stakeholders as a robust solution for real-world\n",
        "biomedical data\n",
        "\n",
        "Answer:-\n",
        "\n",
        "Below is a clear, end-to-end explanation followed by Python code with output, suitable for exams, interviews, and real-world justification.\n",
        "\n",
        "Solution Approach for High-Dimensional Gene Expression Data\n",
        "1Ô∏è Using PCA to Reduce Dimensionality\n",
        "\n",
        "Gene expression datasets have thousands of genes but few samples, leading to overfitting.\n",
        "PCA reduces dimensionality by transforming correlated gene features into fewer orthogonal principal components that retain maximum variance and remove noise.\n",
        "\n",
        "2Ô∏è Deciding How Many Components to Keep\n",
        "\n",
        "Use explained variance ratio\n",
        "\n",
        "Retain components that explain 90‚Äì95% cumulative variance\n",
        "\n",
        "Scree plot or cumulative variance curve helps balance information retention vs complexity\n",
        "\n",
        "3Ô∏è Using KNN After PCA\n",
        "\n",
        "KNN is distance-based and performs poorly in high dimensions\n",
        "\n",
        "PCA makes distances meaningful\n",
        "\n",
        "Train KNN on PCA-transformed data for better generalization and speed\n",
        "\n",
        "4Ô∏è Model Evaluation\n",
        "\n",
        "Use cross-validation\n",
        "\n",
        "Metrics:\n",
        "\n",
        "Accuracy\n",
        "\n",
        "Precision, Recall, F1-score (important in healthcare)\n",
        "\n",
        "Confusion matrix to analyze misclassifications\n",
        "\n",
        "5Ô∏è Justification to Stakeholders\n",
        "\n",
        "‚ÄúThis pipeline reduces noise, prevents overfitting, improves interpretability, and provides stable predictions‚Äîcritical for reliable biomedical decision-making.‚Äù"
      ],
      "metadata": {
        "id": "FbdKZVdz-qru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Simulate high-dimensional gene expression data\n",
        "X, y = make_classification(\n",
        "    n_samples=200,\n",
        "    n_features=1000,\n",
        "    n_informative=50,\n",
        "    n_classes=3,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Apply PCA (retain 95% variance)\n",
        "pca = PCA(n_components=0.95)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "print(\"Number of PCA components retained:\", pca.n_components_)\n",
        "\n",
        "# Train KNN classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_pca, y_train)\n",
        "\n",
        "# Predictions and evaluation\n",
        "y_pred = knn.predict(X_test_pca)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Model Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzoyMY8B-uXB",
        "outputId": "f866a9ee-82a4-4e31-cddb-712bf564baa4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of PCA components retained: 125\n",
            "Model Accuracy: 0.3333333333333333\n"
          ]
        }
      ]
    }
  ]
}