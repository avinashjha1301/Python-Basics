{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1: What is Boosting in Machine Learning? Explain how it improves weak learners.**\n",
        "\n",
        "Answer :- Boosting is an ensemble learning technique in machine learning that combines multiple weak learners to create a strong predictive model. A weak learner is a model that performs only slightly better than random guessing. The key idea behind boosting is to train models sequentially, where each new model focuses more on the data points that were misclassified by the previous models. During training, higher importance or weight is given to these difficult samples so that subsequent learners try harder to correct earlier mistakes. By iteratively reducing errors and combining the predictions of all weak learners, boosting reduces bias and improves overall accuracy, resulting in a robust and highly accurate model."
      ],
      "metadata": {
        "id": "atj8kQ1rm0fr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2: What is the difference between AdaBoost and Gradient Boosting in terms of how models are trained?**\n",
        "\n",
        "Answer:- AdaBoost and Gradient Boosting are both boosting algorithms, but they differ in how models are trained and how errors are handled. In AdaBoost, models are trained sequentially, and after each iteration, misclassified data points are assigned higher weights, forcing the next model to focus more on those difficult samples. The final prediction is a weighted combination of all weak learners based on their performance. In contrast, Gradient Boosting also trains models sequentially but uses a gradient descent approach, where each new model is trained to fit the residual errors (gradients of the loss function) made by the previous ensemble. While AdaBoost directly reweights samples based on misclassification, Gradient Boosting optimizes a specified loss function, making it more flexible and suitable for both regression and classification tasks."
      ],
      "metadata": {
        "id": "KYmpP31Um55w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3: How does regularization help in XGBoost?**\n",
        "\n",
        "Answer:- Regularization in XGBoost helps control model complexity and prevent overfitting by penalizing overly complex trees. XGBoost includes both L1 regularization (alpha) and L2 regularization (lambda) on leaf weights, which discourage large weight values and encourage simpler models. In addition, XGBoost uses tree-specific regularization, such as limiting the number of leaves and penalizing tree depth through a complexity term in its objective function. By adding these penalties to the loss function, XGBoost balances model fit with simplicity, leading to better generalization on unseen data and more stable performance compared to standard gradient boosting methods."
      ],
      "metadata": {
        "id": "9GLQaD11nE7C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4: Why is CatBoost considered efficient for handling categorical data?**\n",
        "\n",
        "Answer:- CatBoost is considered efficient for handling categorical data because it can process categorical features directly without requiring manual encoding such as one-hot encoding. It uses a technique called ordered target statistics, which converts categorical values into numerical representations based on target information while avoiding target leakage. This method preserves useful information from categorical features and reduces overfitting. Additionally, CatBoost employs symmetric (oblivious) decision trees, which make training faster and more stable. By automatically handling missing values and high-cardinality categorical features, CatBoost simplifies preprocessing and delivers strong performance with minimal feature engineering."
      ],
      "metadata": {
        "id": "BdKnqh0pnLme"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5: What are some real-world applications where boosting techniques are preferred over bagging methods? **\n",
        "\n",
        "Answer:- Boosting techniques are preferred over bagging methods in real-world applications where high accuracy and bias reduction are more important than simple variance reduction. Boosting performs especially well on complex problems with structured patterns and when weak learners need to be improved iteratively.\n",
        "\n",
        "Some common real-world applications include:\n",
        "\n",
        "Fraud detection in banking and finance, where boosting models like XGBoost can focus on rare and hard-to-detect fraudulent cases.\n",
        "\n",
        "Credit scoring and risk assessment, where reducing bias and improving prediction precision is critical.\n",
        "\n",
        "Search engines and recommendation systems, where boosting helps rank results more accurately by learning from previous errors.\n",
        "\n",
        "Medical diagnosis, such as cancer detection, where boosting improves classification accuracy on complex medical data.\n",
        "\n",
        "Customer churn prediction and marketing analytics, where boosting captures subtle patterns in customer behavior better than bagging.\n",
        "\n",
        "In these scenarios, boosting is preferred because it learns from mistakes, reduces bias, and achieves higher predictive performance compared to bagging methods like Random Forest."
      ],
      "metadata": {
        "id": "1S-znx6VnS8x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Question 6: Write a Python program to:**\n",
        "● Train an AdaBoost Classifier on the Breast Cancer dataset\n",
        "● Print the model accuracy\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "o3QkGas3nosw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MURCdXxdldEx",
        "outputId": "a9956186-60f1-4454-85d7-2b844a30ef87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost Classifier Accuracy: 0.9707602339181286\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train AdaBoost Classifier\n",
        "ada = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
        "ada.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = ada.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print accuracy\n",
        "print(\"AdaBoost Classifier Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7: Write a Python program to:**\n",
        "● Train a Gradient Boosting Regressor on the California Housing dataset\n",
        "● Evaluate performance using R-squared score\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "0Z7V3nBbnzBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load the California Housing dataset\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train Gradient Boosting Regressor\n",
        "gbr = GradientBoostingRegressor(random_state=42)\n",
        "gbr.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = gbr.predict(X_test)\n",
        "\n",
        "# Evaluate performance using R-squared score\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print R-squared score\n",
        "print(\"Gradient Boosting Regressor R-squared Score:\", r2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1i8RNO8n0tA",
        "outputId": "41ebd20a-a64e-4fea-8712-d158a9d7b512"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting Regressor R-squared Score: 0.7803012822391022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8: Write a Python program to:**\n",
        "● Train an XGBoost Classifier on the Breast Cancer dataset\n",
        "● Tune the learning rate using GridSearchCV\n",
        "● Print the best parameters and accuracy\n",
        "(Include your Python code and output in the code box below.)\n"
      ],
      "metadata": {
        "id": "biCXBpT1n6o8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize XGBoost Classifier\n",
        "xgb = XGBClassifier(\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='logloss',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Define parameter grid for learning rate tuning\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2]\n",
        "}\n",
        "\n",
        "# Apply GridSearchCV\n",
        "grid = GridSearchCV(\n",
        "    estimator=xgb,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy'\n",
        ")\n",
        "\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Get best model\n",
        "best_model = grid.best_estimator_\n",
        "\n",
        "# Make predictions\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print best parameters and accuracy\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"XGBoost Classifier Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZH1UHVTLn8ET",
        "outputId": "ff5a30b2-20fe-494a-ad8a-0dc4c6f00066"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [12:00:09] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [12:00:11] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [12:00:11] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [12:00:12] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [12:00:12] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [12:00:13] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [12:00:13] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [12:00:14] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [12:00:14] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [12:00:14] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [12:00:14] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [12:00:15] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [12:00:15] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [12:00:15] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [12:00:15] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [12:00:15] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [12:00:15] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [12:00:15] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [12:00:15] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'learning_rate': 0.1}\n",
            "XGBoost Classifier Accuracy: 0.9590643274853801\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [12:00:15] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [12:00:15] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9: Write a Python program to:**\n",
        "● Train a CatBoost Classifier\n",
        "● Plot the confusion matrix using seaborn\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "1kxmUYqooexp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from catboost import CatBoostClassifier\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train CatBoost Classifier\n",
        "model = CatBoostClassifier(\n",
        "    iterations=100,\n",
        "    learning_rate=0.1,\n",
        "    depth=6,\n",
        "    verbose=False,\n",
        "    random_state=42\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"CatBoost Classifier Accuracy:\", accuracy)\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot confusion matrix using seaborn\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix - CatBoost Classifier\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "u2n9e6sdogcH",
        "outputId": "250f773c-7ec1-4107-88e7-0b138d7a6866"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'catboost'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-468615913.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcatboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCatBoostClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'catboost'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10: You're working for a FinTech company trying to predict loan default using**\n",
        "customer demographics and transaction behavior.\n",
        "The dataset is imbalanced, contains missing values, and has both numeric and\n",
        "categorical features.\n",
        "Describe your step-by-step data science pipeline using boosting techniques:\n",
        "● Data preprocessing & handling missing/categorical values\n",
        "● Choice between AdaBoost, XGBoost, or CatBoost\n",
        "● Hyperparameter tuning strategy\n",
        "● Evaluation metrics you'd choose and why\n",
        "● How the business would benefit from your model\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "M8yH0cvios4h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report, average_precision_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# 1. Mock Data Setup\n",
        "# Assume 'default' is the target (1 = default, 0 = no default)\n",
        "# Features: 'income' (numeric), 'credit_score' (numeric), 'employment_type' (cat)\n",
        "data = pd.DataFrame({\n",
        "    'income': [50000, 60000, 120000, 30000, np.nan, 85000] * 20,\n",
        "    'credit_score': [600, 750, 800, 550, 620, np.nan] * 20,\n",
        "    'employment_type': ['Full-time', 'Self-employed', 'Full-time', 'Part-time', 'Full-time', 'Unemployed'] * 20,\n",
        "    'default': [1, 0, 0, 1, 0, 1] * 20\n",
        "})\n",
        "\n",
        "X = data.drop('default', axis=1)\n",
        "y = data['default']\n",
        "\n",
        "# 2. Preprocessing Pipeline\n",
        "numeric_features = ['income', 'credit_score']\n",
        "categorical_features = ['employment_type']\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', SimpleImputer(strategy='median'), numeric_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "    ])\n",
        "\n",
        "# 3. Model Definition with Imbalance Handling\n",
        "# scale_pos_weight = count(neg) / count(pos)\n",
        "ratio = float(np.sum(y == 0)) / np.sum(y == 1)\n",
        "\n",
        "model = XGBClassifier(\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='logloss',\n",
        "    scale_pos_weight=ratio\n",
        ")\n",
        "\n",
        "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                      ('classifier', model)])\n",
        "\n",
        "# 4. Hyperparameter Tuning\n",
        "param_grid = {\n",
        "    'classifier__n_estimators': [100, 200],\n",
        "    'classifier__max_depth': [3, 5, 7],\n",
        "    'classifier__learning_rate': [0.01, 0.1, 0.2]\n",
        "}\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "search = RandomizedSearchCV(clf, param_grid, n_iter=5, scoring='average_precision', cv=3)\n",
        "search.fit(X_train, y_train)\n",
        "\n",
        "# 5. Output\n",
        "y_pred = search.predict(X_test)\n",
        "print(f\"Best Parameters: {search.best_params_}\")\n",
        "print(\"\\nModel Evaluation:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(f\"PR-AUC Score: {average_precision_score(y_test, search.predict_proba(X_test)[:, 1]):.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSh1lPNRovMj",
        "outputId": "28740cb6-f6cd-43db-d174-a691015b75d0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [12:04:09] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [12:04:09] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [12:04:09] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [12:04:09] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [12:04:09] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [12:04:09] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [12:04:09] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [12:04:10] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [12:04:10] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [12:04:10] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [12:04:10] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [12:04:10] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [12:04:11] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [12:04:11] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [12:04:11] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'classifier__n_estimators': 200, 'classifier__max_depth': 7, 'classifier__learning_rate': 0.2}\n",
            "\n",
            "Model Evaluation:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        15\n",
            "           1       1.00      1.00      1.00         9\n",
            "\n",
            "    accuracy                           1.00        24\n",
            "   macro avg       1.00      1.00      1.00        24\n",
            "weighted avg       1.00      1.00      1.00        24\n",
            "\n",
            "PR-AUC Score: 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [12:04:11] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        }
      ]
    }
  ]
}